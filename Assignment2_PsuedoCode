// Driver Class 

package second_assgnmnt;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


public class EmpSalDriver extends Configured implements Tool {

  @Override
  public int run(String[] args) throws Exception {

    if (args.length != 3) {
      System.err.printf("Usage: %s [generic options] <input> <output>\n",
          getClass().getSimpleName());
      ToolRunner.printGenericCommandUsage(System.err);
      return -1;
    }
    
    Job job = new Job(getConf(), "Employee Salary Jn");
    job.setJarByClass(getClass());

	MultipleInputs.addInputPath(job, new Path(args[0]), TextInputFormat.class,
	    		EmployeeMapper.class);
	MultipleInputs.addInputPath(job, new Path(args[1]), TextInputFormat.class,
	    		SalaryMapper.class);
    FileOutputFormat.setOutputPath(job, new Path(args[2]));
    
    job.setReducerClass(EmpSalRedcuer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    
    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new EmpSalDriver(), args);
    System.exit(exitCode);
  }
}

// Employee Mapper Class 

package second_assgnmnt;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class EmployeeMapper
  extends Mapper<LongWritable, Text, Text, IntWritable> {

  @Override
  public void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    
    String line = value.toString();
    String empid = line.split(",")[0];
    String empinfo = "A"+ line;

    if (empid != "Id") {
      context.write(new Text(empid), new Text(empinfo));
    }
  }
}

// Salary Mapper Class 

package second_assgnmnt;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class SalaryMapper
  extends Mapper<LongWritable, Text, Text, IntWritable> {

  @Override
  public void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    
    String line = value.toString();
    String empid = line.split(",")[0];
    String salinfo = "B" + line;

    if (empid != "Id") {
      context.write(new Text(empid), new Text(salinfo));
    }
  }
}

// Employee Salary Outer Join Reducer Class

package second_assgnmnt;

import java.io.IOException;
import java.util.ArrayList;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import first_assgnmnt.EmpSalary;
import first_assgnmnt.Employee;
import first_assgnmnt.Salary;

public class EmpSalReducer extends Reducer<Text, Text, Text, Text> {

	@Override
	public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

		ArrayList<Text> listA = new ArrayList<Text>();
		ArrayList<Text> listB = new ArrayList<Text>();

		listA.clear();
		listB.clear();

		while (values.hasNext()) {
			Text tmp = values.next();
			if (tmp.charAt(0) == "A") {

				listA.add(new Text(tmp.toString().substring(1)));

			} else if (tmp.charAt(0) == "B") {
				listB.add(new Text(tmp.toString().substring(1)));
			}

			for (Text emp_rec : listA) {
				if (!listB.isEmpty()) {
					for (Text sal_rec : listB) {
						context.write(A, B);
					}
				} else
					context.write(A, EMPTY_TEXT);
			}
		}

	}

}x

